{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c9b0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in d:\\python_programs\\myenvimport\\lib\\site-packages (0.2.65)\n",
      "Requirement already satisfied: pandas in d:\\python_programs\\myenvimport\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pyyaml in d:\\python_programs\\myenvimport\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: nsepy in d:\\python_programs\\myenvimport\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: yahoofinancials in d:\\python_programs\\myenvimport\\lib\\site-packages (1.20)\n",
      "Requirement already satisfied: matplotlib in d:\\python_programs\\myenvimport\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in d:\\python_programs\\myenvimport\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scipy in d:\\python_programs\\myenvimport\\lib\\site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.31 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (4.13.5)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (6.32.0)\n",
      "Requirement already satisfied: websockets>=13.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python_programs\\myenvimport\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python_programs\\myenvimport\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six in d:\\python_programs\\myenvimport\\lib\\site-packages (from nsepy) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\python_programs\\myenvimport\\lib\\site-packages (from nsepy) (8.2.1)\n",
      "Requirement already satisfied: lxml in d:\\python_programs\\myenvimport\\lib\\site-packages (from nsepy) (6.0.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in d:\\python_programs\\myenvimport\\lib\\site-packages (from yahoofinancials) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\python_programs\\myenvimport\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python_programs\\myenvimport\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in d:\\python_programs\\myenvimport\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in d:\\python_programs\\myenvimport\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
      "Requirement already satisfied: pycparser in d:\\python_programs\\myenvimport\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python_programs\\myenvimport\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python_programs\\myenvimport\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python_programs\\myenvimport\\lib\\site-packages (from requests>=2.31->yfinance) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\python_programs\\myenvimport\\lib\\site-packages (from click->nsepy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance pandas pyyaml nsepy yahoofinancials matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db825c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for January 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_01_2024.yaml\n",
      "Fetching data for February 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_02_2024.yaml\n",
      "Fetching data for March 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_03_2024.yaml\n",
      "Fetching data for April 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_04_2024.yaml\n",
      "Fetching data for May 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_05_2024.yaml\n",
      "Fetching data for June 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_06_2024.yaml\n",
      "Fetching data for July 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_07_2024.yaml\n",
      "Fetching data for August 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_08_2024.yaml\n",
      "Fetching data for September 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_09_2024.yaml\n",
      "Fetching data for October 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_10_2024.yaml\n",
      "Fetching data for November 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_11_2024.yaml\n",
      "Fetching data for December 2024...\n",
      "Saved nifty50_yaml_files\\nifty50_data_12_2024.yaml\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('nifty50_yaml_files', exist_ok=True)\n",
    "\n",
    "# List of Nifty 50 symbols (strings with .NS suffix)\n",
    "NIFTY_50_SYMBOLS = [\n",
    "        \"ADANIENT.NS\",\"ADANIPORTS.NS\",\"APOLLOHOSP.NS\",\"ASIANPAINT.NS\",\"AXISBANK.NS\",\n",
    "        \"BAJAJ-AUTO.NS\",\"BAJFINANCE.NS\",\"BAJAJFINSV.NS\",\"BEL.NS\",\"BHARTIARTL.NS\",\n",
    "        \"BRITANNIA.NS\",\"CIPLA.NS\",\"COALINDIA.NS\",\"DIVISLAB.NS\",\"DRREDDY.NS\",\n",
    "        \"EICHERMOT.NS\",\"GRASIM.NS\",\"HCLTECH.NS\",\"HDFCBANK.NS\",\"HDFCLIFE.NS\",\n",
    "        \"HEROMOTOCO.NS\",\"HINDALCO.NS\",\"ICICIBANK.NS\",\"INFY.NS\",\"INDUSINDBK.NS\",\n",
    "        \"ITC.NS\",\"JSWSTEEL.NS\",\"KOTAKBANK.NS\",\"LT.NS\",\"LTIM.NS\",\"M&M.NS\",\n",
    "        \"MARUTI.NS\",\"NESTLEIND.NS\",\"NTPC.NS\",\"ONGC.NS\",\"POWERGRID.NS\",\n",
    "        \"RELIANCE.NS\",\"SBIN.NS\",\"SBILIFE.NS\",\"SUNPHARMA.NS\",\"TATACONSUM.NS\",\n",
    "        \"TATAMOTORS.NS\",\"TATASTEEL.NS\",\"TCS.NS\",\"TECHM.NS\",\"TITAN.NS\",\n",
    "        \"ULTRACEMCO.NS\",\"UPL.NS\",\"WIPRO.NS\",\"ZEEL.NS\"\n",
    "]\n",
    "\n",
    "# Custom YAML representer for numpy types\n",
    "def numpy_representer(dumper, data):\n",
    "    if isinstance(data, np.integer):\n",
    "        return dumper.represent_int(int(data))\n",
    "    elif isinstance(data, np.floating):\n",
    "        return dumper.represent_float(float(data))\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return dumper.represent_list(data.tolist())\n",
    "    return None\n",
    "\n",
    "yaml.add_representer(np.integer, numpy_representer)\n",
    "yaml.add_representer(np.floating, numpy_representer)\n",
    "yaml.add_representer(np.ndarray, numpy_representer)\n",
    "\n",
    "# Dictionary to cache the sector for each symbol\n",
    "sector_cache = {}\n",
    "\n",
    "def get_sector(symbol):\n",
    "    \"\"\"Fetch the sector for a given stock symbol and cache it.\"\"\"\n",
    "    if symbol in sector_cache:\n",
    "        return sector_cache[symbol]\n",
    "    \n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        sector = stock.info.get('sector', 'N/A')\n",
    "        sector_cache[symbol] = sector\n",
    "        return sector\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sector for {symbol}: {str(e)}\")\n",
    "        return 'N/A'\n",
    "\n",
    "def get_stock_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetch stock data for given symbol and date range.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        hist = stock.history(start=start_date, end=end_date)\n",
    "        sector = get_sector(symbol)\n",
    "        return symbol, hist, sector\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {symbol}: {str(e)}\")\n",
    "        return symbol, None, 'N/A'\n",
    "\n",
    "def process_month(year, month):\n",
    "    \"\"\"Process data for a single month.\"\"\"\n",
    "    start_date = datetime(year, month, 1)\n",
    "    end_date = datetime(year, month, calendar.monthrange(year, month)[1])\n",
    "    \n",
    "    print(f\"Fetching data for {start_date.strftime('%B %Y')}...\")\n",
    "    \n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_symbol = {\n",
    "            executor.submit(\n",
    "                get_stock_data, \n",
    "                symbol, \n",
    "                start_date, \n",
    "                end_date + timedelta(days=1)\n",
    "            ): symbol for symbol in NIFTY_50_SYMBOLS\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol, data, sector = future.result()\n",
    "            if data is not None and not data.empty:\n",
    "                # Convert DataFrame to list of dictionaries\n",
    "                records = []\n",
    "                for date, row in data.iterrows():\n",
    "                    record = {\n",
    "                        'Date': date.strftime('%Y-%m-%d'),\n",
    "                        'Open': float(round(row['Open'], 2)),\n",
    "                        'High': float(round(row['High'], 2)),\n",
    "                        'Low': float(round(row['Low'], 2)),\n",
    "                        'Close': float(round(row['Close'], 2)),\n",
    "                        'Volume': int(row['Volume']),\n",
    "                        'Sector': sector  # Add the sector here\n",
    "                    }\n",
    "                    records.append(record)\n",
    "                results[symbol] = records\n",
    "    \n",
    "    # Save to YAML file with the requested naming format\n",
    "    month_str = str(month).zfill(2)\n",
    "    filename = f\"nifty50_data_{month_str}_{year}.yaml\"\n",
    "    filepath = os.path.join('nifty50_yaml_files', filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        yaml.dump(results, f, sort_keys=False, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Saved {filepath}\")\n",
    "\n",
    "def main():\n",
    "    year = 2024\n",
    "    for month in range(1, 13):\n",
    "        process_month(year, month)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bee6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted nifty50_yaml_files\\nifty50_data_01_2024.yaml to nifty50_csv_files\\nifty50_data_01_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_02_2024.yaml to nifty50_csv_files\\nifty50_data_02_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_03_2024.yaml to nifty50_csv_files\\nifty50_data_03_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_04_2024.yaml to nifty50_csv_files\\nifty50_data_04_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_05_2024.yaml to nifty50_csv_files\\nifty50_data_05_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_06_2024.yaml to nifty50_csv_files\\nifty50_data_06_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_07_2024.yaml to nifty50_csv_files\\nifty50_data_07_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_08_2024.yaml to nifty50_csv_files\\nifty50_data_08_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_09_2024.yaml to nifty50_csv_files\\nifty50_data_09_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_10_2024.yaml to nifty50_csv_files\\nifty50_data_10_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_11_2024.yaml to nifty50_csv_files\\nifty50_data_11_2024.csv\n",
      "Converted nifty50_yaml_files\\nifty50_data_12_2024.yaml to nifty50_csv_files\\nifty50_data_12_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_01_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_02_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_03_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_04_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_05_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_06_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_07_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_08_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_09_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_10_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_11_2024.csv\n",
      "Added data from nifty50_csv_files\\nifty50_data_12_2024.csv\n",
      "Created combined CSV: nifty50_csv_files\\nifty50_full_year_2024.csv\n",
      "Total records: 12300\n",
      "Date range: 2024-01-01 to 2024-12-31\n",
      "Number of unique stocks: 50\n",
      "Conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('nifty50_csv_files', exist_ok=True)\n",
    "\n",
    "# Custom YAML loader to handle numpy types\n",
    "def numpy_constructor(loader, node):\n",
    "    # Convert numpy types to native Python types\n",
    "    value = loader.construct_scalar(node)\n",
    "    try:\n",
    "        # Try to convert to int first, then float\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return value\n",
    "\n",
    "# Add the constructor to handle the specific tag\n",
    "yaml.add_constructor('tag:yaml.org,2002:python/object/apply:numpy._core.multiarray.scalar', \n",
    "                     lambda loader, node: numpy_constructor(loader, node))\n",
    "\n",
    "def yaml_to_csv(nifty50_yaml_file, nifty50_csv_file):\n",
    "    \"\"\"Convert YAML stock data to CSV format, including the 'Sector' column.\"\"\"\n",
    "    try:\n",
    "        # Read YAML file with custom loader\n",
    "        with open(nifty50_yaml_file, 'r') as file:\n",
    "            data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        \n",
    "        # Create a list to hold all rows\n",
    "        all_rows = []\n",
    "        \n",
    "        # Process each stock\n",
    "        for stock, days_data in data.items():\n",
    "            for day_data in days_data:\n",
    "                # Create a row for each day, including the Sector\n",
    "                row = {\n",
    "                    'Stock': stock,\n",
    "                    'Date': day_data['Date'],\n",
    "                    'Open': day_data['Open'],\n",
    "                    'High': day_data['High'],\n",
    "                    'Low': day_data['Low'],\n",
    "                    'Close': day_data['Close'],\n",
    "                    'Volume': day_data['Volume'],\n",
    "                    'Sector': day_data.get('Sector', 'N/A')  # Add the Sector column here \n",
    "                }\n",
    "                all_rows.append(row)\n",
    "        \n",
    "        # Convert to DataFrame and save as CSV\n",
    "        df = pd.DataFrame(all_rows)\n",
    "        df.to_csv(nifty50_csv_file, index=False)\n",
    "        print(f\"Converted {nifty50_yaml_file} to {nifty50_csv_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {nifty50_yaml_file}: {str(e)}\")\n",
    "\n",
    "def convert_all_yaml_files(year=2024):\n",
    "    \"\"\"Convert all YAML files for a given year to CSV.\"\"\"\n",
    "    yaml_dir = 'nifty50_yaml_files'\n",
    "    csv_dir = 'nifty50_csv_files'\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        month_str = str(month).zfill(2)  # Format month as two digits\n",
    "        yaml_filename = f\"nifty50_data_{month_str}_{year}.yaml\"\n",
    "        csv_filename = f\"nifty50_data_{month_str}_{year}.csv\"\n",
    "        \n",
    "        nifty50_yaml_file = os.path.join(yaml_dir, yaml_filename)\n",
    "        nifty50_csv_file = os.path.join(csv_dir, csv_filename)\n",
    "        \n",
    "        if os.path.exists(nifty50_yaml_file):\n",
    "            yaml_to_csv(nifty50_yaml_file, nifty50_csv_file)\n",
    "        else:\n",
    "            print(f\"File {nifty50_yaml_file} not found\")\n",
    "\n",
    "def create_combined_csv(year=2024):\n",
    "    \"\"\"Create a combined CSV with all months' data.\"\"\"\n",
    "    csv_dir = 'nifty50_csv_files'\n",
    "    all_data = []\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        month_str = str(month).zfill(2)  # Format month as two digits\n",
    "        nifty50_csv_file = os.path.join(csv_dir, f\"nifty50_data_{month_str}_{year}.csv\")\n",
    "        \n",
    "        if os.path.exists(nifty50_csv_file):\n",
    "            df = pd.read_csv(nifty50_csv_file)\n",
    "            all_data.append(df)\n",
    "            print(f\"Added data from {nifty50_csv_file}\")\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        nifty50_combined_csv = os.path.join(csv_dir, f\"nifty50_full_year_{year}.csv\")\n",
    "        combined_df.to_csv(nifty50_combined_csv, index=False)\n",
    "        print(f\"Created combined CSV: {nifty50_combined_csv}\")\n",
    "        \n",
    "        # Display some statistics\n",
    "        print(f\"Total records: {len(combined_df)}\")\n",
    "        print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "        print(f\"Number of unique stocks: {combined_df['Stock'].nunique()}\")\n",
    "    else:\n",
    "        print(\"No CSV files found to combine.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Convert all YAML files to CSV\n",
    "    convert_all_yaml_files(2024)\n",
    "    \n",
    "    # Create a combined CSV with all data\n",
    "    create_combined_csv(2024)\n",
    "    \n",
    "    print(\"Conversion completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cfd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (12300, 8)\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADANIPORTS.NS</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1017.22</td>\n",
       "      <td>1048.33</td>\n",
       "      <td>1013.70</td>\n",
       "      <td>1038.32</td>\n",
       "      <td>3989711</td>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADANIPORTS.NS</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>1038.32</td>\n",
       "      <td>1073.05</td>\n",
       "      <td>1021.72</td>\n",
       "      <td>1068.59</td>\n",
       "      <td>6344621</td>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADANIPORTS.NS</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>1094.95</td>\n",
       "      <td>1133.60</td>\n",
       "      <td>1053.09</td>\n",
       "      <td>1084.30</td>\n",
       "      <td>33060778</td>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADANIPORTS.NS</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>1106.84</td>\n",
       "      <td>1120.72</td>\n",
       "      <td>1093.52</td>\n",
       "      <td>1112.99</td>\n",
       "      <td>9771995</td>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADANIPORTS.NS</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>1118.83</td>\n",
       "      <td>1149.45</td>\n",
       "      <td>1115.17</td>\n",
       "      <td>1143.75</td>\n",
       "      <td>10622789</td>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stock        Date     Open     High      Low    Close    Volume  \\\n",
       "0  ADANIPORTS.NS  2024-01-01  1017.22  1048.33  1013.70  1038.32   3989711   \n",
       "1  ADANIPORTS.NS  2024-01-02  1038.32  1073.05  1021.72  1068.59   6344621   \n",
       "2  ADANIPORTS.NS  2024-01-03  1094.95  1133.60  1053.09  1084.30  33060778   \n",
       "3  ADANIPORTS.NS  2024-01-04  1106.84  1120.72  1093.52  1112.99   9771995   \n",
       "4  ADANIPORTS.NS  2024-01-05  1118.83  1149.45  1115.17  1143.75  10622789   \n",
       "\n",
       "        Sector  \n",
       "0  Industrials  \n",
       "1  Industrials  \n",
       "2  Industrials  \n",
       "3  Industrials  \n",
       "4  Industrials  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load the combined CSV file\n",
    "# Assuming you've run the previous scripts and have the combined CSV file\n",
    "combined_csv_path = r\"D:\\python_programs\\NIFTY_50 STOCKS_ANALYSIS\\nifty50_csv_files\\nifty50_full_year_2024.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types before conversion:\n",
      "Stock      object\n",
      "Date       object\n",
      "Open      float64\n",
      "High      float64\n",
      "Low       float64\n",
      "Close     float64\n",
      "Volume      int64\n",
      "Sector     object\n",
      "dtype: object\n",
      "\n",
      "Data types after conversion:\n",
      "Stock             object\n",
      "Date      datetime64[ns]\n",
      "Open             float64\n",
      "High             float64\n",
      "Low              float64\n",
      "Close            float64\n",
      "Volume             int64\n",
      "Sector            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types and convert if necessary\n",
    "print(\"Data types before conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Ensure numeric columns are properly formatted\n",
    "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e94d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Stock     0\n",
      "Date      0\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "Sector    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values percentage:\n",
      "Stock     0.0\n",
      "Date      0.0\n",
      "Open      0.0\n",
      "High      0.0\n",
      "Low       0.0\n",
      "Close     0.0\n",
      "Volume    0.0\n",
      "Sector    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nMissing values percentage:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d70ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning:\n",
      "Stock     0\n",
      "Date      0\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "Sector    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balamurugan\\AppData\\Local\\Temp\\ipykernel_12716\\2118513386.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_cleaned = df_cleaned.groupby('Stock').apply(\n"
     ]
    }
   ],
   "source": [
    "#  Handle missing values\n",
    "# For OHLC data, we can forward fill then backward fill\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Group by stock and fill missing values within each stock\n",
    "df_cleaned = df_cleaned.groupby('Stock').apply(\n",
    "    lambda group: group.ffill().bfill()\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Check if any missing values remain\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af60945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Number of duplicate rows after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "#  Check for duplicates\n",
    "print(\"Number of duplicate rows:\", df_cleaned.duplicated().sum())\n",
    "\n",
    "# Remove duplicates if any\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "print(\"Number of duplicate rows after cleaning:\", df_cleaned.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with negative Open prices: 0\n",
      "Rows with negative High prices: 0\n",
      "Rows with negative Low prices: 0\n",
      "Rows with negative Close prices: 0\n",
      "Rows with negative Volume: 0\n",
      "Rows where High < Low: 0\n",
      "Rows with inconsistent OHLC values: 0\n"
     ]
    }
   ],
   "source": [
    "#  Validate data consistency\n",
    "# Check for negative prices or volumes\n",
    "print(\"Rows with negative Open prices:\", (df_cleaned['Open'] <= 0).sum())\n",
    "print(\"Rows with negative High prices:\", (df_cleaned['High'] <= 0).sum())\n",
    "print(\"Rows with negative Low prices:\", (df_cleaned['Low'] <= 0).sum())\n",
    "print(\"Rows with negative Close prices:\", (df_cleaned['Close'] <= 0).sum())\n",
    "print(\"Rows with negative Volume:\", (df_cleaned['Volume'] < 0).sum())\n",
    "\n",
    "# Check if High is greater than or equal to Low\n",
    "inconsistent_high_low = (df_cleaned['High'] < df_cleaned['Low']).sum()\n",
    "print(\"Rows where High < Low:\", inconsistent_high_low)\n",
    "\n",
    "# Check if Open and Close are between High and Low\n",
    "inconsistent_ohlc = ((df_cleaned['Open'] > df_cleaned['High']) | \n",
    "                     (df_cleaned['Open'] < df_cleaned['Low']) |\n",
    "                     (df_cleaned['Close'] > df_cleaned['High']) | \n",
    "                     (df_cleaned['Close'] < df_cleaned['Low'])).sum()\n",
    "print(\"Rows with inconsistent OHLC values:\", inconsistent_ohlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e280849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where High < Low after cleaning: 0\n",
      "Rows with inconsistent OHLC values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "#  Handle inconsistent data\n",
    "# Fix rows where High < Low by swapping them\n",
    "mask = df_cleaned['High'] < df_cleaned['Low']\n",
    "df_cleaned.loc[mask, ['High', 'Low']] = df_cleaned.loc[mask, ['Low', 'High']].values\n",
    "\n",
    "# Ensure Open and Close are within High and Low range\n",
    "df_cleaned['Open'] = np.where(df_cleaned['Open'] > df_cleaned['High'], df_cleaned['High'], df_cleaned['Open'])\n",
    "df_cleaned['Open'] = np.where(df_cleaned['Open'] < df_cleaned['Low'], df_cleaned['Low'], df_cleaned['Open'])\n",
    "df_cleaned['Close'] = np.where(df_cleaned['Close'] > df_cleaned['High'], df_cleaned['High'], df_cleaned['Close'])\n",
    "df_cleaned['Close'] = np.where(df_cleaned['Close'] < df_cleaned['Low'], df_cleaned['Low'], df_cleaned['Close'])\n",
    "\n",
    "# Verify the fixes\n",
    "inconsistent_high_low = (df_cleaned['High'] < df_cleaned['Low']).sum()\n",
    "inconsistent_ohlc = ((df_cleaned['Open'] > df_cleaned['High']) | \n",
    "                     (df_cleaned['Open'] < df_cleaned['Low']) |\n",
    "                     (df_cleaned['Close'] > df_cleaned['High']) | \n",
    "                     (df_cleaned['Close'] < df_cleaned['Low'])).sum()\n",
    "print(\"Rows where High < Low after cleaning:\", inconsistent_high_low)\n",
    "print(\"Rows with inconsistent OHLC values after cleaning:\", inconsistent_ohlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeae19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: cleaned_data/nifty50_cleaned_2024.csv\n",
      "Final dataset shape: (12300, 8)\n"
     ]
    }
   ],
   "source": [
    "#  Save the cleaned data\n",
    "# Create a new directory for cleaned data\n",
    "os.makedirs('cleaned_data', exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataframe\n",
    "cleaned_csv_path = \"cleaned_data/nifty50_cleaned_2024.csv\"\n",
    "df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to: {cleaned_csv_path}\")\n",
    "print(f\"Final dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12300, 8)\n",
      "Columns: ['Stock', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Sector']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2849.69</td>\n",
       "      <td>2944.31</td>\n",
       "      <td>2839.45</td>\n",
       "      <td>2914.53</td>\n",
       "      <td>2898619</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>2919.33</td>\n",
       "      <td>2945.51</td>\n",
       "      <td>2838.40</td>\n",
       "      <td>2929.72</td>\n",
       "      <td>2671368</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>3044.22</td>\n",
       "      <td>3196.08</td>\n",
       "      <td>2952.90</td>\n",
       "      <td>3000.26</td>\n",
       "      <td>19725411</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>3036.62</td>\n",
       "      <td>3041.67</td>\n",
       "      <td>2987.27</td>\n",
       "      <td>2995.56</td>\n",
       "      <td>2975620</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>3007.20</td>\n",
       "      <td>3064.10</td>\n",
       "      <td>2978.38</td>\n",
       "      <td>3003.85</td>\n",
       "      <td>3219949</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Stock        Date     Open     High      Low    Close    Volume  \\\n",
       "0  ADANIENT.NS  2024-01-01  2849.69  2944.31  2839.45  2914.53   2898619   \n",
       "1  ADANIENT.NS  2024-01-02  2919.33  2945.51  2838.40  2929.72   2671368   \n",
       "2  ADANIENT.NS  2024-01-03  3044.22  3196.08  2952.90  3000.26  19725411   \n",
       "3  ADANIENT.NS  2024-01-04  3036.62  3041.67  2987.27  2995.56   2975620   \n",
       "4  ADANIENT.NS  2024-01-05  3007.20  3064.10  2978.38  3003.85   3219949   \n",
       "\n",
       "   Sector  \n",
       "0  Energy  \n",
       "1  Energy  \n",
       "2  Energy  \n",
       "3  Energy  \n",
       "4  Energy  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load the cleaned data\n",
    "cleaned_data_path = \"cleaned_data/nifty50_cleaned_2024.csv\"\n",
    "df = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f26c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stocks: 50\n",
      "Stock names:\n",
      "1. ADANIENT.NS\n",
      "2. ADANIPORTS.NS\n",
      "3. APOLLOHOSP.NS\n",
      "4. ASIANPAINT.NS\n",
      "5. AXISBANK.NS\n",
      "6. BAJAJ-AUTO.NS\n",
      "7. BAJAJFINSV.NS\n",
      "8. BAJFINANCE.NS\n",
      "9. BEL.NS\n",
      "10. BHARTIARTL.NS\n",
      "11. BRITANNIA.NS\n",
      "12. CIPLA.NS\n",
      "13. COALINDIA.NS\n",
      "14. DIVISLAB.NS\n",
      "15. DRREDDY.NS\n",
      "16. EICHERMOT.NS\n",
      "17. GRASIM.NS\n",
      "18. HCLTECH.NS\n",
      "19. HDFCBANK.NS\n",
      "20. HDFCLIFE.NS\n",
      "21. HEROMOTOCO.NS\n",
      "22. HINDALCO.NS\n",
      "23. ICICIBANK.NS\n",
      "24. INDUSINDBK.NS\n",
      "25. INFY.NS\n",
      "26. ITC.NS\n",
      "27. JSWSTEEL.NS\n",
      "28. KOTAKBANK.NS\n",
      "29. LT.NS\n",
      "30. LTIM.NS\n",
      "31. M&M.NS\n",
      "32. MARUTI.NS\n",
      "33. NESTLEIND.NS\n",
      "34. NTPC.NS\n",
      "35. ONGC.NS\n",
      "36. POWERGRID.NS\n",
      "37. RELIANCE.NS\n",
      "38. SBILIFE.NS\n",
      "39. SBIN.NS\n",
      "40. SUNPHARMA.NS\n",
      "41. TATACONSUM.NS\n",
      "42. TATAMOTORS.NS\n",
      "43. TATASTEEL.NS\n",
      "44. TCS.NS\n",
      "45. TECHM.NS\n",
      "46. TITAN.NS\n",
      "47. ULTRACEMCO.NS\n",
      "48. UPL.NS\n",
      "49. WIPRO.NS\n",
      "50. ZEEL.NS\n"
     ]
    }
   ],
   "source": [
    "#  Get unique stock names\n",
    "unique_stocks = df['Stock'].unique()\n",
    "print(f\"Number of unique stocks: {len(unique_stocks)}\")\n",
    "print(\"Stock names:\")\n",
    "for i, stock in enumerate(unique_stocks):\n",
    "    print(f\"{i+1}. {stock}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e895339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ADANIENT.NS data to ADANIENT_2024_OHLCV.csv\n",
      "Saved ADANIPORTS.NS data to ADANIPORTS_2024_OHLCV.csv\n",
      "Saved APOLLOHOSP.NS data to APOLLOHOSP_2024_OHLCV.csv\n",
      "Saved ASIANPAINT.NS data to ASIANPAINT_2024_OHLCV.csv\n",
      "Saved AXISBANK.NS data to AXISBANK_2024_OHLCV.csv\n",
      "Saved BAJAJ-AUTO.NS data to BAJAJ_AUTO_2024_OHLCV.csv\n",
      "Saved BAJAJFINSV.NS data to BAJAJFINSV_2024_OHLCV.csv\n",
      "Saved BAJFINANCE.NS data to BAJFINANCE_2024_OHLCV.csv\n",
      "Saved BEL.NS data to BEL_2024_OHLCV.csv\n",
      "Saved BHARTIARTL.NS data to BHARTIARTL_2024_OHLCV.csv\n",
      "Saved BRITANNIA.NS data to BRITANNIA_2024_OHLCV.csv\n",
      "Saved CIPLA.NS data to CIPLA_2024_OHLCV.csv\n",
      "Saved COALINDIA.NS data to COALINDIA_2024_OHLCV.csv\n",
      "Saved DIVISLAB.NS data to DIVISLAB_2024_OHLCV.csv\n",
      "Saved DRREDDY.NS data to DRREDDY_2024_OHLCV.csv\n",
      "Saved EICHERMOT.NS data to EICHERMOT_2024_OHLCV.csv\n",
      "Saved GRASIM.NS data to GRASIM_2024_OHLCV.csv\n",
      "Saved HCLTECH.NS data to HCLTECH_2024_OHLCV.csv\n",
      "Saved HDFCBANK.NS data to HDFCBANK_2024_OHLCV.csv\n",
      "Saved HDFCLIFE.NS data to HDFCLIFE_2024_OHLCV.csv\n",
      "Saved HEROMOTOCO.NS data to HEROMOTOCO_2024_OHLCV.csv\n",
      "Saved HINDALCO.NS data to HINDALCO_2024_OHLCV.csv\n",
      "Saved ICICIBANK.NS data to ICICIBANK_2024_OHLCV.csv\n",
      "Saved INDUSINDBK.NS data to INDUSINDBK_2024_OHLCV.csv\n",
      "Saved INFY.NS data to INFY_2024_OHLCV.csv\n",
      "Saved ITC.NS data to ITC_2024_OHLCV.csv\n",
      "Saved JSWSTEEL.NS data to JSWSTEEL_2024_OHLCV.csv\n",
      "Saved KOTAKBANK.NS data to KOTAKBANK_2024_OHLCV.csv\n",
      "Saved LT.NS data to LT_2024_OHLCV.csv\n",
      "Saved LTIM.NS data to LTIM_2024_OHLCV.csv\n",
      "Saved M&M.NS data to M&M_2024_OHLCV.csv\n",
      "Saved MARUTI.NS data to MARUTI_2024_OHLCV.csv\n",
      "Saved NESTLEIND.NS data to NESTLEIND_2024_OHLCV.csv\n",
      "Saved NTPC.NS data to NTPC_2024_OHLCV.csv\n",
      "Saved ONGC.NS data to ONGC_2024_OHLCV.csv\n",
      "Saved POWERGRID.NS data to POWERGRID_2024_OHLCV.csv\n",
      "Saved RELIANCE.NS data to RELIANCE_2024_OHLCV.csv\n",
      "Saved SBILIFE.NS data to SBILIFE_2024_OHLCV.csv\n",
      "Saved SBIN.NS data to SBIN_2024_OHLCV.csv\n",
      "Saved SUNPHARMA.NS data to SUNPHARMA_2024_OHLCV.csv\n",
      "Saved TATACONSUM.NS data to TATACONSUM_2024_OHLCV.csv\n",
      "Saved TATAMOTORS.NS data to TATAMOTORS_2024_OHLCV.csv\n",
      "Saved TATASTEEL.NS data to TATASTEEL_2024_OHLCV.csv\n",
      "Saved TCS.NS data to TCS_2024_OHLCV.csv\n",
      "Saved TECHM.NS data to TECHM_2024_OHLCV.csv\n",
      "Saved TITAN.NS data to TITAN_2024_OHLCV.csv\n",
      "Saved ULTRACEMCO.NS data to ULTRACEMCO_2024_OHLCV.csv\n",
      "Saved UPL.NS data to UPL_2024_OHLCV.csv\n",
      "Saved WIPRO.NS data to WIPRO_2024_OHLCV.csv\n",
      "Saved ZEEL.NS data to ZEEL_2024_OHLCV.csv\n",
      "\n",
      "Created 50 CSV files in the 'stocks_data_2024' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define OHLCV columns (adjust if needed)\n",
    "ohlcv_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# Folder where CSVs will be stored\n",
    "stocks_dir = \"stocks_data_2024\"\n",
    "os.makedirs(stocks_dir, exist_ok=True)\n",
    "\n",
    "# Step 6: Create separate CSV files for each stock\n",
    "for stock in unique_stocks:\n",
    "    # Filter data for the current stock\n",
    "    stock_data = df[df['Stock'] == stock]\n",
    "    \n",
    "    # Select only OHLCV columns\n",
    "    stock_data_ohlcv = stock_data[ohlcv_columns]\n",
    "    \n",
    "    # Create filename (replace special characters if needed)\n",
    "    safe_stock_name = stock.replace('.NS', '').replace('-', '_').replace(' ', '_')\n",
    "    filename = f\"{safe_stock_name}_2024_OHLCV.csv\"\n",
    "    filepath = os.path.join(stocks_dir, filename)\n",
    "    \n",
    "    # Save to CSV\n",
    "    stock_data_ohlcv.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"Saved {stock} data to {filename}\")\n",
    "\n",
    "print(f\"\\nCreated {len(unique_stocks)} CSV files in the '{stocks_dir}' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc09967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 CSV files:\n",
      "  - ADANIENT_2024_OHLCV.csv\n",
      "  - ADANIPORTS_2024_OHLCV.csv\n",
      "  - APOLLOHOSP_2024_OHLCV.csv\n",
      "  - ASIANPAINT_2024_OHLCV.csv\n",
      "  - AXISBANK_2024_OHLCV.csv\n",
      "  - ... and 45 more\n"
     ]
    }
   ],
   "source": [
    "#  Verify the created files\n",
    "import glob\n",
    "\n",
    "# List all CSV files in the stocks directory\n",
    "csv_files = glob.glob(os.path.join(stocks_dir, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "\n",
    "for file in sorted(csv_files)[:5]:  # Show first 5 files\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  - ... and {len(csv_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: ADANIENT_2024_OHLCV.csv\n",
      "Shape: (246, 6)\n",
      "\n",
      "First few rows:\n",
      "         Date     Open     High      Low    Close    Volume\n",
      "0  2024-01-01  2849.69  2944.31  2839.45  2914.53   2898619\n",
      "1  2024-01-02  2919.33  2945.51  2838.40  2929.72   2671368\n",
      "2  2024-01-03  3044.22  3196.08  2952.90  3000.26  19725411\n",
      "3  2024-01-04  3036.62  3041.67  2987.27  2995.56   2975620\n",
      "4  2024-01-05  3007.20  3064.10  2978.38  3003.85   3219949\n",
      "\n",
      "Date range:\n",
      "From: 2024-01-01\n",
      "To: 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "#  Sample one of the created files to verify content\n",
    "if csv_files:\n",
    "    sample_file = csv_files[0]\n",
    "    sample_data = pd.read_csv(sample_file)\n",
    "    \n",
    "    print(f\"Sample file: {os.path.basename(sample_file)}\")\n",
    "    print(f\"Shape: {sample_data.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(sample_data.head())\n",
    "    \n",
    "    print(\"\\nDate range:\")\n",
    "    print(f\"From: {sample_data['Date'].min()}\")\n",
    "    print(f\"To: {sample_data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for: ADANIENT.NS\n",
      "Retrieved 246 records\n",
      "         Date     Open     High      Low    Close    Volume\n",
      "0  2024-01-01  2849.69  2944.31  2839.45  2914.53   2898619\n",
      "1  2024-01-02  2919.33  2945.51  2838.40  2929.72   2671368\n",
      "2  2024-01-03  3044.22  3196.08  2952.90  3000.26  19725411\n",
      "3  2024-01-04  3036.62  3041.67  2987.27  2995.56   2975620\n",
      "4  2024-01-05  3007.20  3064.10  2978.38  3003.85   3219949\n"
     ]
    }
   ],
   "source": [
    "#  Create a function to get data for a specific stock\n",
    "def get_stock_data(stock_name, data_directory=stocks_dir):\n",
    "    \"\"\"\n",
    "    Retrieve OHLCV data for a specific stock\n",
    "    \n",
    "    Parameters:\n",
    "    stock_name (str): Name of the stock (with or without .NS extension)\n",
    "    data_directory (str): Directory where stock CSV files are stored\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: OHLCV data for the requested stock\n",
    "    \"\"\"\n",
    "    # Standardize stock name format\n",
    "    if not stock_name.endswith('.NS'):\n",
    "        stock_name += '.NS'\n",
    "    \n",
    "    # Create filename\n",
    "    safe_stock_name = stock_name.replace('.NS', '').replace('-', '_').replace(' ', '_')\n",
    "    filename = f\"{safe_stock_name}_2024_OHLCV.csv\"\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Data not found for {stock_name}\")\n",
    "        print(\"Available stocks:\")\n",
    "        for stock in unique_stocks:\n",
    "            print(f\"  - {stock}\")\n",
    "        return None\n",
    "    \n",
    "    # Load and return data\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "# Example usage\n",
    "if csv_files:\n",
    "    sample_stock = unique_stocks[0]\n",
    "    print(f\"Getting data for: {sample_stock}\")\n",
    "    stock_data = get_stock_data(sample_stock)\n",
    "    if stock_data is not None:\n",
    "        print(f\"Retrieved {len(stock_data)} records\")\n",
    "        print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for: ['ADANIENT.NS', 'ADANIPORTS.NS', 'APOLLOHOSP.NS']\n",
      "Retrieved data for 3 stocks\n",
      "ADANIENT.NS: 246 records\n",
      "ADANIPORTS.NS: 246 records\n",
      "APOLLOHOSP.NS: 246 records\n"
     ]
    }
   ],
   "source": [
    "#  Create a function to get multiple stocks at once\n",
    "def get_multiple_stocks(stock_list, data_directory=stocks_dir):\n",
    "    \"\"\"\n",
    "    Retrieve OHLCV data for multiple stocks\n",
    "    \n",
    "    Parameters:\n",
    "    stock_list (list): List of stock names\n",
    "    data_directory (str): Directory where stock CSV files are stored\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with stock names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    for stock in stock_list:\n",
    "        data = get_stock_data(stock, data_directory)\n",
    "        if data is not None:\n",
    "            result[stock] = data\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if len(unique_stocks) >= 3:\n",
    "    sample_stocks = [unique_stocks[0], unique_stocks[1], unique_stocks[2]]\n",
    "    print(f\"Getting data for: {sample_stocks}\")\n",
    "    multiple_data = get_multiple_stocks(sample_stocks)\n",
    "    \n",
    "    print(f\"Retrieved data for {len(multiple_data)} stocks\")\n",
    "    for stock, data in multiple_data.items():\n",
    "        print(f\"{stock}: {len(data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f74578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd185507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvimport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
